{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import re\n",
    "import hickle as hkl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.legacy.nn import Reshape\n",
    "import graphviz\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from visualize import make_dot\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imresize, imread, imshow\n",
    "import time\n",
    "import logging\n",
    "from math import log,sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dense161 = models.densenet161(pretrained=True)\n",
    "#dense161"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for param in model5.parameters():\n",
    "    print param.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c1 = 0\n",
    "for para in dense161.parameters():\n",
    "    c1 += para.numel()\n",
    "c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deconvolution2 = nn.Sequential(nn.BatchNorm2d(2208),nn.ReLU(),nn.ConvTranspose2d(2208,512,kernel_size=(5,5),stride=(2,2),padding=(1,1)),\n",
    "                             nn.BatchNorm2d(512),nn.ReLU(),nn.ConvTranspose2d(512,256,kernel_size=(5,5),stride=(2,2),padding=(1,1)),\n",
    "                             nn.BatchNorm2d(256),nn.ReLU(),nn.ConvTranspose2d(256,128,kernel_size=(5,5),stride=(2,2),padding=(1,1)),\n",
    "                             nn.BatchNorm2d(128),nn.ReLU(),nn.ConvTranspose2d(128,64,kernel_size=(5,5),stride=(2,2),padding=(1,1)),\n",
    "                             nn.BatchNorm2d(64), nn.ReLU(),nn.ConvTranspose2d(64,1,kernel_size=(5,5),stride=(2,2),padding=(1,1))\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "deconvolution1 = nn.Sequential(nn.BatchNorm2d(2208),nn.ReLU(),nn.Conv2d(2208,512,1),\n",
    "                             nn.BatchNorm2d(512),nn.ReLU(),nn.ConvTranspose2d(512,256,kernel_size=(5,5),stride=(2,2),padding=(1,1)),\n",
    "                             nn.BatchNorm2d(256),nn.ReLU(),nn.ConvTranspose2d(256,128,kernel_size=(5,5),stride=(2,2),padding=(1,1)),\n",
    "                             nn.BatchNorm2d(128),nn.ReLU(),nn.ConvTranspose2d(128,64,kernel_size=(5,5),stride=(2,2),padding=(1,1)),\n",
    "                             nn.BatchNorm2d(64), nn.ReLU(),nn.ConvTranspose2d(64,1,kernel_size=(5,5),stride=(2,2),padding=(1,1))\n",
    "                              )#nn.BatchNorm2d(64),nn.ReLU(),nn.Conv2d(64,1,1)\n",
    "                             #)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deconvolution = nn.Sequential(nn.BatchNorm2d(2208),nn.ReLU(),nn.Conv2d(2208,512,1),nn.BatchNorm2d(512),nn.ReLU(),nn.Conv2d(512,256,1),\n",
    "                             nn.BatchNorm2d(256),nn.ReLU(),nn.ConvTranspose2d(256,128,kernel_size=(5,5),stride=(2,2),padding=(1,1)),\n",
    "                             nn.BatchNorm2d(128),nn.ReLU(),nn.ConvTranspose2d(128,64,kernel_size=(5,5),stride=(2,2),padding=(1,1)),\n",
    "                             nn.BatchNorm2d(64),nn.ReLU(),nn.ConvTranspose2d(64,1,kernel_size=(5,5),stride=(2,2),padding=(1,1))\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for m in deconvolution1.modules():\n",
    "    print type(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for m in deconvolution1.modules():\n",
    "    if isinstance(m,nn.Conv2d):\n",
    "        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "        m.weight.data.normal_(0,sqrt(2./n))\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        m.weight.data.fill_(1)\n",
    "        m.bias.data.zero_()\n",
    "    elif isinstance(m, nn.ConvTranspose2d):\n",
    "        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "        m.weight.data.normal_(0,sqrt(2./n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp1 = nn.Sequential(*list(dense161.children())[:-1])\n",
    "temp2 = temp1.children()\n",
    "\n",
    "temp3 = nn.Sequential(*list(temp1.children())[0])\n",
    "temp3\n",
    "temp4 = nn.Sequential(*list(temp3.children())[0:12])\n",
    "print(temp4)\n",
    "temp4.forward(Variable(torch.randn(2,3,240,320))).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b1 = nn.Sequential(*list(dense161.children())[:-1])\n",
    "b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MyModel5(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super(MyModel5, self).__init__()\n",
    "        self.pretrained_model = nn.Sequential(*list(dense161.children())[:-1])\n",
    "        self.deconv1 = deconvolution1\n",
    "        \n",
    "        #self.unpool1 = nn.UpsamplingBilinear2d(scale_factor=2)\n",
    "        #self.unpool2 = nn.UpsamplingBilinear2d(size=(60,80))\n",
    "        self.conv0 = nn.Conv2d(2208,512,1)\n",
    "        #self.conv1 = nn.Conv2d(512,256,1)\n",
    "        \n",
    "        #self.tconv1 = nn.ConvTranspose2d(256,128,kernel_size=(5,5),stride=(2,2),padding=(1,1))\n",
    "        #self.tconv2 = nn.ConvTranspose2d(128,64,kernel_size=(5,5),stride=(2,2),padding=(1,1))\n",
    "        #self.tconv3 = nn.ConvTranspose2d(64,1,kernel_size=(5,5),stride=(2,2),padding=(1,1))\n",
    "        \n",
    "        #self.tconv2 = nn.ConvTranspose2d(128,64,3,stride=2,padding=0)\n",
    "        #self.tconv3 = nn.ConvTranspose2d(64,1,3,stride=2,padding=0)\n",
    "        #self.conv1 = nn.Conv2d(1024,512,1)\n",
    "        #self.conv1 = nn.Conv2d(1024,512,5,padding=(2,2))\n",
    "        #self.conv2 = nn.Conv2d(512,256,5,padding=(2,2))\n",
    "        #self.conv3 = nn.Conv2d(256,128,5,padding=(4,2))\n",
    "        #self.conv4 = nn.Conv2d(128,1,3,padding=(1,1))\n",
    "        #self.conv5 = nn.Conv2d(128,1,3,padding=(1,1))\n",
    "        #self.bnorm = nn.BatchNorm2d(1024)\n",
    "        self.bnorm1 = nn.BatchNorm2d(2208)\n",
    "        #self.bnorm2 = nn.BatchNorm2d(512)\n",
    "        #self.bnorm3 = nn.BatchNorm2d(256)\n",
    "        #self.bnorm4 = nn.BatchNorm2d(128)\n",
    "        #self.bnorm5 = nn.BatchNorm2d(64)\n",
    "        \n",
    "    def unpool_conv(self,inp1,conv,unpool):\n",
    "        #x = inp1\n",
    "        x = unpool(inp1)\n",
    "        x = F.relu(conv(x))\n",
    "        return x\n",
    "    \n",
    "    def newconv(self,inp1,conv,bn1):\n",
    "        size1 = inp1.size()[1]\n",
    "        #print('inside newconv ',size1)\n",
    "          \n",
    "        x = bn1(inp1)\n",
    "        x = F.relu(x)\n",
    "        x = conv(x)\n",
    "        return x\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pretrained_model(x)\n",
    "        #print(x.size()[1])\n",
    "\n",
    "        #x = self.newconv(x,self.conv0,self.bnorm1)\n",
    "        #x = self.newconv(x,self.conv1,self.bnorm2)\n",
    "        #x = self.newconv(x,self.tconv1,self.bnorm3)\n",
    "        #x = self.newconv(x,self.tconv2,self.bnorm4)\n",
    "        #x = self.newconv(x,self.tconv3,self.bnorm5)\n",
    "        #x = self.unpool_conv(x,self.conv1,self.unpool1)\n",
    "        #x = self.unpool_conv(x,self.conv2,self.unpool1)\n",
    "        #x = self.unpool_conv(x,self.conv3,self.unpool1)\n",
    "        #x = self.unpool_conv(x,self.conv4,self.unpool2)\n",
    "        #x = self.unpool2(x)\n",
    "        #x = self.conv4(x)\n",
    "        \n",
    "        x = self.deconv1(x)\n",
    "        x = x.view(-1,127,175)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 127, 175])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5 = MyModel5(dense161)\n",
    "model5.forward(Variable(torch.randn(2,3,240,320))).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model6 = MyModel5(dense161)\n",
    "model6.forward(Variable(torch.randn(2,3,240,320))).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for params in model5.parameters():\n",
    "    print(params.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum1 = 0\n",
    "        \n",
    "print(\"Number of layers ---> \",len(list(model5.parameters())))\n",
    "for params in model5.parameters():\n",
    "    if params.requires_grad == True:\n",
    "        sum1 += params.numel()\n",
    "    \n",
    "print(\"Total number of parameters ---> \",sum1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('xtest shape ', (134, 3, 240, 320))\n",
      "('ytest shape ', (134, 127, 175))\n",
      "('xtrain shape ', (400, 3, 240, 320))\n",
      "('ytrain shape ', (400, 127, 175))\n"
     ]
    }
   ],
   "source": [
    "#dataset loading part...\n",
    "\n",
    "#file1 = h5py.File('data/make3dTraining.h5')\n",
    "#file2 = h5py.File('data/make3dTest1.h5')\n",
    "#file3 = h5py.File('data/make3dTest2.h5')\n",
    "\n",
    "file1 = h5py.File('data/Make3DTrain.h5')\n",
    "file2 = h5py.File('data/Make3DTest.h5')\n",
    "\n",
    "xtrain1 = np.array(file1['xtrain1'],dtype=np.float32)\n",
    "ytrain1 = np.array(file1['ytrain1'],dtype=np.float32)\n",
    "\n",
    "\n",
    "xtest1 = np.array(file2['xtrain1'],dtype=np.float32)\n",
    "ytest1 = np.array(file2['ytrain1'],dtype=np.float32)\n",
    "#ytest1 = np.moveaxis(ytest1,[0,1,2],[2,1,0])\n",
    "#xtest1 = np.moveaxis(xtest1,[0,1,2,3],[1,3,2,0])\n",
    "\n",
    "print(\"xtest shape \",xtest1.shape)\n",
    "print(\"ytest shape \",ytest1.shape)\n",
    "print(\"xtrain shape \",xtrain1.shape)\n",
    "print(\"ytrain shape \",ytrain1.shape)\n",
    "#xtrain1 /= 255\n",
    "#xtest1 /= 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file2 = h5py.File('data/nyuV2testdata240new.h5')\n",
    "xtest1 = np.array(file2['xtrain1'],dtype=np.float32)\n",
    "ytest1 = np.array(file2['ytrain1'],dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xtrain1 = np.clip(xtrain1,0.0,255.0)\n",
    "print(\"xtrain shape \",xtrain1.shape)\n",
    "print(\"max \",np.min(xtrain1))\n",
    "print(\"min \",np.max(xtrain1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xtrain1 /= 255\n",
    "xtest1 /= 255\n",
    "\n",
    "mean1 = [0.485,0.456,0.406]\n",
    "std1 = [0.229,0.224,0.225]\n",
    "\n",
    "#xtrain1[:,0,:,:] = (xtrain1[:,0,:,:]-mean1[0])/std1[0]\n",
    "#xtrain1[:,1,:,:] = (xtrain1[:,1,:,:]-mean1[1])/std1[1]\n",
    "#xtrain1[:,2,:,:] = (xtrain1[:,2,:,:]-mean1[2])/std1[2]\n",
    "\n",
    "xtest1[:,0,:,:] = (xtest1[:,0,:,:]-mean1[0])/std1[0]\n",
    "xtest1[:,1,:,:] = (xtest1[:,1,:,:]-mean1[1])/std1[1]\n",
    "xtest1[:,2,:,:] = (xtest1[:,2,:,:]-mean1[2])/std1[2]\n",
    "\n",
    "#print(\"max in xtrain ---> \",np.max(xtrain1))\n",
    "#print(\"min in xtrain ---> \",np.min(xtrain1))\n",
    "print(\"max in xtest  ---> \",np.max(xtest1))\n",
    "print(\"min in xtest  ---> \",np.min(xtest1))\n",
    "\n",
    "#print(\"xtrain shape ---> \",xtrain1.shape)\n",
    "#print(\"ytrain shape ---> \",ytrain1.shape)\n",
    "print(\"xtest shape ---> \",xtest1.shape)\n",
    "print(\"ytest shape ---> \",ytest1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.max(xtrain1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#center crop  with flipped bits from left to right..fil1\n",
    "\n",
    "#file1 = h5py.File('data/xnewtrain1.h5')\n",
    "#file2 = h5py.File('data/ynewtrain1.h5')\n",
    "\n",
    "#xtrain1 = np.array(file1['xtrain1'],dtype=np.float32)\n",
    "#ytrain1 = np.array(file2['ytrain1'],dtype=np.float32)\n",
    "\n",
    "\n",
    "\n",
    "#xtrainfinal = np.zeros((1590,3,240,320),dtype=np.float32)\n",
    "#ytrainfinal = np.zeros((1590,60,80),dtype=np.float32)\n",
    "#ytrainfinal = np.zeros((1590,63,87),dtype=np.float32)\n",
    "\n",
    "#xtrainfinal[:795,:,:,:] = xtrain1\n",
    "#xtrainfinal[795:,:,:,:] = np.fliplr(xtrain1)\n",
    "\n",
    "#ytrainfinal[:795,:,:] = ytrain1\n",
    "#ytrainfinal[795:,:,:] = np.fliplr(ytrain1)\n",
    "\n",
    "\n",
    "#print(\"xtrainfinal shape --> \",xtrainfinal.shape)\n",
    "#print(\"ytrainfinal shape --> \",ytrainfinal.shape)\n",
    "print(\"max in xtrain ---> \",np.max(xtrain1))\n",
    "print(\"min in xtrain ---> \",np.min(xtrain1))\n",
    "print(\"max in xtest  ---> \",np.max(xtest1))\n",
    "print(\"min in xtest  ---> \",np.min(xtest1))\n",
    "\n",
    "xtrain1 /= 255\n",
    "#xtrainfinal /= 255\n",
    "xtest1 /= 255\n",
    "\n",
    "mean1 = [0.485,0.456,0.406]\n",
    "std1 = [0.229,0.224,0.225]\n",
    "\n",
    "xtrain1[:,0,:,:] = (xtrain1[:,0,:,:]-mean1[0])/std1[0]\n",
    "xtrain1[:,1,:,:] = (xtrain1[:,1,:,:]-mean1[1])/std1[1]\n",
    "xtrain1[:,2,:,:] = (xtrain1[:,2,:,:]-mean1[2])/std1[2]\n",
    "\n",
    "#xtrainfinal[:,0,:,:] = (xtrainfinal[:,0,:,:]-mean1[0])/std1[0]\n",
    "#xtrainfinal[:,1,:,:] = (xtrainfinal[:,1,:,:]-mean1[1])/std1[1]\n",
    "#xtrainfinal[:,2,:,:] = (xtrainfinal[:,2,:,:]-mean1[2])/std1[2]\n",
    "\n",
    "xtest1[:,0,:,:] = (xtest1[:,0,:,:]-mean1[0])/std1[0]\n",
    "xtest1[:,1,:,:] = (xtest1[:,1,:,:]-mean1[1])/std1[1]\n",
    "xtest1[:,2,:,:] = (xtest1[:,2,:,:]-mean1[2])/std1[2]\n",
    "\n",
    "print(\"max in xtrain ---> \",np.max(xtrain1))\n",
    "print(\"min in xtrain ---> \",np.min(xtrain1))\n",
    "print(\"max in xtest  ---> \",np.max(xtest1))\n",
    "print(\"min in xtest  ---> \",np.min(xtest1))\n",
    "\n",
    "print(\"xtrain shape ---> \",xtrain1.shape)\n",
    "print(\"ytrain shape ---> \",ytrain1.shape)\n",
    "print(\"xtest shape ---> \",xtest1.shape)\n",
    "print(\"ytest shape ---> \",ytest1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.nan in xtrain2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, loss, optimizer, x_val, y_val):\n",
    "    x = Variable(x_val,requires_grad = False).cuda()\n",
    "    y = Variable(y_val,requires_grad = False).cuda()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    fx = model.forward(x)\n",
    "    output = loss.forward(fx,y)\n",
    "    output.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    return output.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xtrain2 = torch.from_numpy(xtrain1).float()\n",
    "xtest2 = torch.from_numpy(xtest1).float()\n",
    "ytrain2 = torch.from_numpy(ytrain1).float()\n",
    "ytest2 = torch.from_numpy(ytest1).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for param in model5.parameters():\n",
    "    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#xtrainfinal2 = torch.from_numpy(xtrainfinal).float()\n",
    "#ytrainfinal2 = torch.from_numpy(ytrainfinal).float()\n",
    "\n",
    "ytest2.size()\n",
    "#np.min(xtrain1[0])\n",
    "#xtrainfinal2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#custom loss function.... this will be reverse Huber...\n",
    "\n",
    "class BerhuLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BerhuLoss, self).__init__()\n",
    "        \n",
    "    def forward(self,input, target):\n",
    "        #target is the ground truth value....\n",
    "        mask1 = target.le(70)\n",
    "        mask1 = mask1.float()\n",
    "        count1 = mask1.eq(0).sum().float()\n",
    "        \n",
    "        target = mask1 * target\n",
    "        input = mask1 * input\n",
    "        \n",
    "        if count1 == 0.0:\n",
    "            count1 = 111125.0\n",
    "        \n",
    "        \n",
    "        diff = torch.abs(target-input)\n",
    "        batch_size, lossval = 5, 0.0\n",
    "        c = 0.2 * torch.max(diff)\n",
    "        c1 = c.data[0]\n",
    "        temp = diff > c1\n",
    "        check1 = torch.prod(temp)\n",
    "        \n",
    "        if check1 == 0:\n",
    "            #if all elements in diff matrix are not greater than scalar c.\n",
    "            lossval = torch.sum(diff)\n",
    "            lossval = lossval/count1\n",
    "        else:\n",
    "            temp4 = torch.pow(diff,2)\n",
    "            d = torch.pow(c,2)\n",
    "            temp4 = temp4.add(d.expand_as(temp4))\n",
    "            lossval = torch.sum(temp4/(2*c))\n",
    "            lossval = lossval/count1\n",
    "        return lossval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t1 = Variable(torch.Tensor(5,127,175))\n",
    "t2 = Variable(torch.Tensor(5,127,175))\n",
    "t1 = t1.cuda()\n",
    "t2 = t2.cuda()\n",
    "\n",
    "b = BerhuLoss()\n",
    "\n",
    "time1 = time.time()\n",
    "samp = b.forward(t1,t2)\n",
    "time2 = time.time() - time1\n",
    "\n",
    "print(\"time \",time2)\n",
    "samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#custom loss function....\n",
    "\n",
    "class SquaredL2Norm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SquaredL2Norm, self).__init__()\n",
    "        \n",
    "    def forward(self,input, target):\n",
    "        diff = torch.abs(target-input)\n",
    "        #temp1 = torch.sqrt(diff**2)\n",
    "        temp1 = torch.sqrt(torch.sum(diff**2))\n",
    "        return temp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#custom loss function....\n",
    "\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RMSELoss, self).__init__()\n",
    "        \n",
    "    def forward(self,input, target):\n",
    "        mask = target.le(70)\n",
    "        \n",
    "        mask1 = mask.float()\n",
    "        count1 = mask1.eq(0).sum().float()\n",
    "        target = mask1 * target        \n",
    "        input = mask1 * input\n",
    "        \n",
    "        if count1 == 0:\n",
    "            count1 = 111125.0\n",
    "        \n",
    "        diff = torch.abs(target-input)\n",
    "        temp1 = diff**2\n",
    "        temp2 = torch.sum(temp1)\n",
    "        temp2 = temp2/count1\n",
    "        return torch.sqrt(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def optim_scheduler2(model,optimizer,epoch, init_lr=0.01, lr_decay_epoch=12):\n",
    "    #lr = init_lr * (0.1**(epoch // lr_decay_epoch))\n",
    "    #lr = 0.01\n",
    "    #if epoch % lr_decay_epoch == 0:\n",
    "    #    print('LR is set to {}'.format(lr))  \n",
    "    #if epoch>20 and epoch<51:\n",
    "    #    lr = 0.001\n",
    "    #elif epoch>=50 and epoch<=100:\n",
    "    #   lr = 0.0001\n",
    "    #else:\n",
    "    #    pass\n",
    "    \n",
    "    model6 = MyModel5(dense161)\n",
    "    model6 = model6.cuda()\n",
    "    model6.load_state_dict(torch.load(\"modeltest.pth\"))\n",
    "    \n",
    "    for params in optimizer.param_groups:\n",
    "        lr1 = params['lr']\n",
    "    \n",
    "    lr = lr1/5\n",
    "    \n",
    "    print(\"New learning rate ---> \",lr)\n",
    "    optimizer = optim.SGD(model6.parameters(), lr=lr, momentum=0.9)\n",
    "    return optimizer,model6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this code adds augmentation done online..\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "seq1 = iaa.Sequential([\n",
    "        iaa.Fliplr(1)\n",
    "])\n",
    "\n",
    "seq2 = iaa.Sequential([\n",
    "        iaa.Affine(rotate=(-5,5))\n",
    "])\n",
    "\n",
    "seq3 = iaa.Sequential([\n",
    "        iaa.Multiply((0.8,1.2))\n",
    "])\n",
    "\n",
    "seq1 = seq1.to_deterministic()\n",
    "seq2 = seq2.to_deterministic()\n",
    "seq3 = seq3.to_deterministic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t1 = xtest1[0:5]\n",
    "t4 = ytest1[0:5]\n",
    "t1 = np.moveaxis(t1,[0,1,2,3],[0,3,1,2])\n",
    "print(\"shape now \",t1.shape)\n",
    "imshow(t1[1])\n",
    "print(np.min(t1[1]))\n",
    "t2 = seq1.augment_images(t1)\n",
    "t6 = seq1.augment_images(t4)\n",
    "imshow(t2[1])\n",
    "imshow(t6[1])\n",
    "print(np.min(t2[1]))\n",
    "t1 == t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.randint(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model5 = nn.DataParallel(model5, device_ids=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model5.load_state_dict(torch.load(\"make3dreports/BerhuTrain_129.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#MUST UNCOMMENT BELOW LINE...\n",
    "model5 = model5.cuda()\n",
    "\n",
    "#loading the model after the weights of epoch50.. to check what loss the model gives if lr is taken as 0.0001\n",
    "optimizer = optim.SGD(model5.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "#optimizer = optim.SGD(model5.deconv1.parameters(), lr=0.001, momentum=0.9)\n",
    "#optimizer = optim.SGD(model5.parameters(), lr=0.001, momentum=0.9)\n",
    "#criterion = SquaredL2Norm()\n",
    "\n",
    "criterion = BerhuLoss()\n",
    "#criterion = RMSELoss()\n",
    "\n",
    "currepochloss = float('Inf')\n",
    "epochs, n_examples, i, batch_size, flag = 150,400, 0, 5, 0\n",
    "tester = 0\n",
    "saveme = 1 \n",
    "while i != epochs:\n",
    "    since = time.time()\n",
    "    cost, batchloss = 0.0, 0.0\n",
    "    num_batches = n_examples/batch_size\n",
    "    \n",
    "    indices = np.random.permutation(400)\n",
    "    #batchwise training starts here...\n",
    "    for k in range(num_batches):\n",
    "        since1 = time.time()\n",
    "        \n",
    "        xtrain3 = torch.FloatTensor(batch_size,3,240,320)\n",
    "        ytrain3 = torch.FloatTensor(batch_size,127,175)\n",
    "        \n",
    "        for ind in range(batch_size):\n",
    "            #ind1 = np.random.randint(0,5599)\n",
    "            ind1 = np.random.randint(0,399)\n",
    "            newxind = indices[ind1]\n",
    "            xtrain3[ind,:,:,:] = xtrain2[newxind,:,:,:]\n",
    "            ytrain3[ind,:,:] = ytrain2[newxind,:,:]\n",
    "        \n",
    "        guess = np.random.randint(4)\n",
    "        \n",
    "        if guess == 0:\n",
    "            #print(\"here1\")\n",
    "            store1 = xtrain3.numpy()\n",
    "            store2 = ytrain3.numpy()\n",
    "            store1 = np.moveaxis(store1,[0,1,2,3],[0,3,1,2])\n",
    "            out1 = seq1.augment_images(store1)\n",
    "            out2 = seq1.augment_images(store2)\n",
    "            out1 = np.moveaxis(out1,[0,1,2,3],[0,2,3,1])\n",
    "            xtrain3 = torch.from_numpy(out1).float()\n",
    "            ytrain3 = torch.from_numpy(out2).float()\n",
    "        elif guess == 2:\n",
    "            #print(\"here2\")\n",
    "            store1 = xtrain3.numpy()\n",
    "            store2 = ytrain3.numpy()\n",
    "            store1 = np.moveaxis(store1,[0,1,2,3],[0,3,1,2])\n",
    "            out1 = seq2.augment_images(store1)\n",
    "            out2 = seq2.augment_images(store2)\n",
    "            out1 = np.moveaxis(out1,[0,1,2,3],[0,2,3,1])\n",
    "            xtrain3 = torch.from_numpy(out1).float()\n",
    "            ytrain3 = torch.from_numpy(out2).float()\n",
    "        elif guess == 3:\n",
    "            #print(\"here3\")\n",
    "            store1 = xtrain3.numpy()\n",
    "            store2 = ytrain3.numpy()\n",
    "            store1 = np.moveaxis(store1,[0,1,2,3],[0,3,1,2])\n",
    "            out1 = seq3.augment_images(store1)\n",
    "            out2 = seq3.augment_images(store2)\n",
    "            out1 = np.moveaxis(out1,[0,1,2,3],[0,2,3,1])\n",
    "            xtrain3 = torch.from_numpy(out1).float()\n",
    "            ytrain3 = torch.from_numpy(out2).float()\n",
    "            \n",
    "        \n",
    "            \n",
    "        batchloss = train(model5,criterion, optimizer, xtrain3, ytrain3)\n",
    "    \n",
    "        batch_time = time.time() - since1\n",
    "        cost += batchloss\n",
    "        #str1 = 'Loss = {:.4f} for batch number = {:d} at epoch {:d} completed in {:.0f}m {:.0f}s'.format(batchloss,k,(i+1),(batch_time//60),(batch_time%60))\n",
    "        #logger1.info(str1)\n",
    "        #print('Loss = {:.4f} for batch number = {:d} at epoch {:d} completed in {:.0f}m {:.0f}s'.format(batchloss,k,(i+1),(batch_time//60),(batch_time%60)))\n",
    "        #print(\"Loss for batch number \"+str(k)+\" is \" +str(batchloss)+\" and takes \"+str(batch_time//60)+\"m \"+str(batch_time%60)+\"s\")\n",
    "    time_elapsed = time.time() - since\n",
    "    epochloss = cost/num_batches\n",
    "    \n",
    "    if epochloss < currepochloss:\n",
    "        torch.save(model5.state_dict(),\"make3dreports/Make3dBerhu.pth\")\n",
    "        flag = 0\n",
    "        currepochloss = epochloss\n",
    "    else:\n",
    "        flag += 1\n",
    "        \n",
    "        if flag == 6:\n",
    "            for p in optimizer.param_groups:\n",
    "                lr2 = p['lr']\n",
    "            newlr = lr2/5\n",
    "            if newlr < 1e-7:\n",
    "                print(\"Cant decrease further!!\")\n",
    "                newlr = 1e-7\n",
    "            flag = 0 \n",
    "            optimizer = optim.SGD(model5.parameters(), lr=newlr, momentum=0.9)\n",
    "            print(\"Learning rate changed from \"+str(lr2)+\" to \"+str(newlr))\n",
    "            \n",
    "        print(\"Loss \"+str(epochloss)+\" is bigger than Loss \"+str(currepochloss)+\" in the prev epoch \")\n",
    "    \n",
    "    #str2 = 'Loss = {:.4f} at epoch {:d} completed in {:.0f}m {:.0f}s'.format((cost/num_batches),(i+1),(time_elapsed//60),(time_elapsed%60))\n",
    "    #logger2.info(str2)\n",
    "    print('Loss = {:.4f} at epoch {:d} completed in {:.0f}m {:.0f}s'.format(epochloss,(i+1),(time_elapsed//60),(time_elapsed%60)))\n",
    "    i += 1 \n",
    "\n",
    "    testchecker()\n",
    "    torch.save(model5.state_dict(),\"make3dreports/newreport\"+str(saveme)+\".pth\")\n",
    "    saveme += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#torch.save(model5.state_dict(),\"RMSE5974withRMSE3524samp-lrnow001-10to20to50to30epochs.pth\")\n",
    "torch.save(model5.state_dict(),\"RMSE5748epochs120-140Berhu.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model6 = MyModel5(dense161)\n",
    "model6 = nn.DataParallel(model6, device_ids=[0,1])\n",
    "#model6.load_state_dict(torch.load(\"pth_Weights/RMSE5748epochs120-140Berhu.pth\"))\n",
    "#model6.load_state_dict(torch.load(\"data/Berhu8724samples_test_7.pth\"))\n",
    "model6.load_state_dict(torch.load(\"make3dreports/BerhuTrain_129.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for params in optimizer.param_groups:\n",
    "    print params['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmse_calc2(ytrue, ypred):\n",
    "    mask = ytrue.le(70)\n",
    "    mask1 = mask.float()\n",
    "    count1 = mask1.eq(0).sum().float()\n",
    "    ytrue = mask1 * ytrue        \n",
    "    ypred = mask1 * ypred\n",
    "        \n",
    "    if count1 == 0.0:\n",
    "        count1 = 111125.0\n",
    "        #count1 = 44450\n",
    "        \n",
    "    diff = torch.abs(ytrue-ypred)\n",
    "    temp1 = diff**2\n",
    "    temp2 = torch.sum(temp1)\n",
    "    temp2 = temp2/count1\n",
    "    return torch.sqrt(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rmse calculation function..\n",
    "\n",
    "def rmse_calc(ytrue, ypred):\n",
    "    mask = ytrue.le(70)\n",
    "    mask1 = mask.float()\n",
    "    count1 = mask1.eq(0).sum().float()\n",
    "    ytrue = mask1 * ytrue        \n",
    "    ypred = mask1 * ypred\n",
    "        \n",
    "    if count1 == 0.0:\n",
    "        #count1 = 111125.0\n",
    "        count1 = 44450\n",
    "        \n",
    "    diff = torch.abs(ytrue-ypred)\n",
    "    temp1 = diff**2\n",
    "    temp2 = torch.sum(temp1)\n",
    "    temp2 = temp2/count1\n",
    "    return torch.sqrt(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def testchecker():\n",
    "\n",
    "    #testing of the architecture...\n",
    "    test_loss = 0\n",
    "    rmse_loss = 0\n",
    "    num_batches = 0\n",
    "    curr_rmse=0\n",
    "\n",
    "    #xtest1 = xtest1.cuda()\n",
    "    #ytest1 = ytest1.cuda()\n",
    "\n",
    "    #6 evenly divides the test batch size..\n",
    "    test_batch_size = 2\n",
    "    test_epochs = 1\n",
    "    n_examples = 134\n",
    "    #n_examples = 795\n",
    "    count = 0\n",
    "    array1 = [0]*67\n",
    "    finalpred = Variable(torch.zeros((n_examples,127,175)))\n",
    "    #print(\"finalpred size is ---> \", finalpred.size())\n",
    "\n",
    "    for i in range(test_epochs):\n",
    "        cost = 0\n",
    "        num_batches = n_examples/test_batch_size\n",
    "        #print(\"num of batches --->\", num_batches)\n",
    "        for k in range(num_batches):\n",
    "            start, end = k*test_batch_size, (k+1)*test_batch_size\n",
    "            output = model5.forward(Variable(xtest2[start:end], volatile=True).cuda())\n",
    "            finalpred[start:end] = output\n",
    "            #print(\"output size is ---> \",output.size(),\" count is --->\",count, \" start is -->\", start, \" end is -->\", end)\n",
    "            count += 1\n",
    "            curr_rmse = rmse_calc(Variable(ytest2[start:end]).cuda(), output)\n",
    "            array1[k] = curr_rmse.data[0]\n",
    "            rmse_loss += curr_rmse\n",
    "            #print(\"curr rmse = %f with batch number = %d\" % (curr_rmse.data.cpu().numpy(),k))\n",
    "\n",
    "        print(\"Test RMSE cost = %f\", rmse_loss/num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#testing of the architecture...\n",
    "test_loss = 0\n",
    "rmse_loss = 0\n",
    "num_batches = 0\n",
    "curr_rmse=0\n",
    "\n",
    "#model6 = model6.cuda()\n",
    "\n",
    "#xtest1 = xtest1.cuda()\n",
    "#ytest1 = ytest1.cuda()\n",
    "\n",
    "#6 evenly divides the test batch size..\n",
    "test_batch_size = 2\n",
    "test_epochs = 1\n",
    "n_examples = 134\n",
    "#n_examples = 795\n",
    "count = 0\n",
    "array1 = [0]*67\n",
    "finalpred = Variable(torch.zeros((n_examples,127,175)))\n",
    "print(\"finalpred size is ---> \", finalpred.size())\n",
    "\n",
    "for i in range(test_epochs):\n",
    "    cost = 0\n",
    "    num_batches = n_examples/test_batch_size\n",
    "    print(\"num of batches --->\", num_batches)\n",
    "    timer1 = time.time()\n",
    "    for k in range(num_batches):\n",
    "        start, end = k*test_batch_size, (k+1)*test_batch_size\n",
    "        output = model5.forward(Variable(xtest2[start:end], volatile=True).cuda())\n",
    "        finalpred[start:end] = output\n",
    "        #print(\"output size is ---> \",output.size(),\" count is --->\",count, \" start is -->\", start, \" end is -->\", end)\n",
    "        count += 1\n",
    "        curr_rmse = rmse_calc(Variable(ytest2[start:end]).cuda(), output)\n",
    "        array1[k] = curr_rmse.data[0]\n",
    "        rmse_loss += curr_rmse\n",
    "        print(\"curr rmse = %f with batch number = %d\" % (curr_rmse.data.cpu().numpy(),k))\n",
    "    finish = time.time() - timer1   \n",
    "    print(\"RMSE cost = %f\", rmse_loss/num_batches)\n",
    "    print(\"min --> \",finish//60)\n",
    "    print(\"sec --> \",finish%60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = torch.rand(2,2)\n",
    "\n",
    "p = d.numpy()\n",
    "\n",
    "#Variables are wrappers around tensors that save operation history. the data can accessed with below method\n",
    "data1 = finalpred.data.numpy()\n",
    "print(data1.shape)\n",
    "\n",
    "#sxtest2 = xtest1.numpy()\n",
    "#sytest2 = ytest1.numpy()\n",
    "\n",
    "#print(sxtest2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.min(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(data1[79][0][100])\n",
    "print(ytest1[79][0][100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to make the background go blue.... the indices where ground truth is more than 70, it can be replaced with 0\n",
    "# and 0 in the prediction (data) for the same indices... should work...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ytest5 = ytest1.copy()\n",
    "inds2 = np.where(ytest5 > 70.0)\n",
    "#print(inds2)\n",
    "ytest5[inds2[0],inds2[1],inds2[2]] = 0.0\n",
    "data1[inds2[0],inds2[1],inds2[2]] = 0.0\n",
    "#print(ytest2[0])\n",
    "ytest1[0]\n",
    "\n",
    "#ytest2[ytest2 > 70.0] = 0.0\n",
    "#ytest2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#output display using matplotlib and bokeh... ONLY FOR TEST IMAGES\n",
    "\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "plt.rcParams['image.interpolation'] = 'none'\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(9)\n",
    "fig.set_figwidth(9)\n",
    "ind = 0\n",
    "flag = 0\n",
    "\n",
    "#ytest3 = ytestmod.numpy()\n",
    "ytest3 = ytest5\n",
    "#imageinds = [79,80,81,82,83]\n",
    "imageinds = [64,32,84,63,87]\n",
    "\n",
    "for i in range(1,21,4):\n",
    "    nind = imageinds[ind]\n",
    "    img1 = np.zeros((240,320,3))\n",
    "    temp = xtest1[nind]\n",
    "    #print(finxtest1[5].shape)\n",
    "\n",
    "    img1[:,:,0] = temp[0,:,:]\n",
    "    img1[:,:,1] = temp[1,:,:]\n",
    "    img1[:,:,2] = temp[2,:,:]\n",
    "    img1 = imresize(img1,(240,320,3))\n",
    "    \n",
    "    a=fig.add_subplot(5,4,i)\n",
    "    imgplot = plt.imshow(img1)\n",
    "    if flag == 0:\n",
    "        a.set_title('Input')\n",
    "    a.axes.get_xaxis().set_visible(False)\n",
    "    a.axes.get_yaxis().set_visible(False)\n",
    "    a=fig.add_subplot(5,4,i+1)\n",
    "    ##################################################\n",
    "    imgplot = plt.imshow(imresize(data1[nind],(480,640)))\n",
    "    imgplot.set_cmap('jet')\n",
    "    #print(\"hi inside\")\n",
    "    if flag == 0:\n",
    "        a.set_title('Predicted Depth')\n",
    "    a.axes.get_xaxis().set_visible(False)\n",
    "    a.axes.get_yaxis().set_visible(False)\n",
    "    ##################################################\n",
    "    a=fig.add_subplot(5,4,i+2)\n",
    "    imgplot = plt.imshow(ytest3[nind])\n",
    "    imgplot.set_cmap('jet')\n",
    "    if flag == 0:\n",
    "        a.set_title('Actual Depth')\n",
    "    a.axes.get_xaxis().set_visible(False)\n",
    "    a.axes.get_yaxis().set_visible(False)\n",
    "    ##################################################\n",
    "    a=fig.add_subplot(5,4,i+3)\n",
    "    error1 = np.abs(data1[nind]-ytest3[nind])\n",
    "    #print \"max is --->\", error1.max()\n",
    "    #print \"min is --->\", error1.min()\n",
    "    imgplot = plt.imshow(error1)\n",
    "    if flag == 0:\n",
    "        a.set_title('Absolute Error')\n",
    "    #imgplot.set_rotation(0)\n",
    "    a.axes.get_xaxis().set_visible(False)\n",
    "    a.axes.get_yaxis().set_visible(False)\n",
    "    #imgplot.set_cmap('hot')\n",
    "    imgplot.set_cmap('jet')\n",
    "    plt.colorbar(fraction=0.080, pad=0.03)\n",
    "    #plt.colorbar(fraction=0.080, pad=0.03,ticks=[0.0,0.2,0.4,0.6])\n",
    "    #plt.clim(0,1)\n",
    "    plt.clim(vmin=0, vmax=0.7)\n",
    "    ##################################################\n",
    "    ind += 1\n",
    "    flag = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = ytest2[1]\n",
    "mask1 = ytest2[1].le(70).float()\n",
    "sample\n",
    "mask1*sample\n",
    "mask1.eq(0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#absolute error\n",
    "\n",
    "def abs_error(ytrue, ypred):\n",
    "    #ytrue is the actual values of the depth map..\n",
    "    #ypred is the predicted values of the depth map from the modified model..\n",
    "    \n",
    "    mask = ytrue.le(70)\n",
    "    #print(mask)\n",
    "    mask1 = mask.float()\n",
    "    #print(\"type \",type(mask1))\n",
    "    count1 = mask1.eq(0).sum()\n",
    "    print(\"count is \",count1)\n",
    "    ytrue = mask1 * ytrue        \n",
    "    ypred = mask1 * ypred\n",
    "    \n",
    "    if count1 == 0.0:\n",
    "        count1 = 2978150.0\n",
    "    \n",
    "    diff = torch.abs(ytrue-ypred)\n",
    "    #print(diff)\n",
    "    temp1 = diff/ytrue\n",
    "    temp2 = torch.sum(temp1)\n",
    "    temp2 = temp2/count1\n",
    "    return temp2\n",
    "\n",
    "def abs_error1(ytrue,ypred):\n",
    "    #print(\"ytrue size \",ytrue.shape)\n",
    "    inds = np.where(ytrue < 70.0)\n",
    "    print(len(inds[0]))\n",
    "    print(inds[0])\n",
    "    #print(\"inds size \",inds.shape)\n",
    "    ytrue = ytrue[inds[0],inds[1],inds[2]]\n",
    "    ypred = ypred[inds[0],inds[1],inds[2]]\n",
    "    rel = np.abs(ytrue-ypred)/ytrue\n",
    "    rel = np.mean(rel)\n",
    "    return rel\n",
    "\n",
    "def sq_error(ytrue,ypred):\n",
    "    diff = torch.abs(ytrue-ypred)\n",
    "    diff = diff**2\n",
    "    temp1 = diff/ytrue\n",
    "    temp2 = torch.mean(temp1)\n",
    "    return temp2\n",
    "\n",
    "def log_rmse(ytrue,ypred):\n",
    "    #print(\"type --> \",type(ytrue))\n",
    "    inds = np.where(ytrue < 70.0)\n",
    "    #print(\"inds \",inds)\n",
    "    ytrue = ytrue[inds[0],inds[1],inds[2]]\n",
    "    ypred = ypred[inds[0],inds[1],inds[2]]\n",
    "    diff = abs(np.log10(ytrue)-np.log10(ypred))\n",
    "    #temp1 = diff**2\n",
    "    print(\"num elements \",len(ypred))\n",
    "    temp2 = np.mean(diff)\n",
    "    return temp2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final1 = finalpred.data.numpy()\n",
    "final1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ans1 = abs_error1(ytest1,data1)\n",
    "print(\"abs error --> \",ans1)\n",
    "\n",
    "print(\"type is \",type(ytest2))\n",
    "\n",
    "ans3 = log_rmse(ytest1,data1)\n",
    "print(\"log rmse error --> \",ans3)\n",
    "\n",
    "#per1,per2,per3 = calc_threshold(ytest2,finalpred.data,654)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(per1)\n",
    "print(per2)\n",
    "print(per3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(per1)\n",
    "print(per2)\n",
    "print(per3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.ndimage import zoom, rotate\n",
    "\n",
    "def rand_aug(im,z):\n",
    "    \n",
    "    ang = np.float32(np.random.random(1)*10.0-5.0); ang = ang[0]\n",
    "    print(\"angle is ---> \",ang)\n",
    "    rgb = np.float32(np.random.random(3)*0.4+0.8);\n",
    "    print(\"rgb value --> \",rgb)\n",
    "    \n",
    "    ctrst = np.float32(np.random.random(1)+0.5); ctrst = ctrst[0]\n",
    "    print(\"ctrss --> \",ctrst)\n",
    "\n",
    "    im = rotate(im,ang,reshape=False,mode='nearest',order=1).copy()\n",
    "    z = rotate(z,ang,reshape=False,mode='nearest',order=1).copy()\n",
    "    print(\"im shape ---> \",im.shape)\n",
    "\n",
    "      \n",
    "    for j in range(3):\n",
    "        im[:,:,j] = im[:,:,j] * rgb[j]\n",
    "    im[...] = np.minimum(255.0,(im[...] ** ctrst) * (255.0**(1.0-ctrst)))\n",
    "    \n",
    "    return im,z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "im,z = rand_aug(xtest1[0],ytest1[0])\n",
    "np.min(im)\n",
    "im[...].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def color1(im):\n",
    "    rgb = np.float32(np.random.random(3)*0.4+0.8)\n",
    "    im1 = np.zeros((240,320,3),dtype=np.float32)\n",
    "    for j in range(3):\n",
    "            im1[:,:,j] = im[:,:,j] * rgb[j]\n",
    "            \n",
    "    ctrst = np.float32(np.random.random(1)+0.5); ctrst = ctrst[0]\n",
    "    im1[...] = np.minimum(255.0,(im1[...] ** ctrst) * (255.0**(1.0-ctrst)))\n",
    "    return im1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t1 = xtest1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample1 = xtest1[0]\n",
    "#hold = np.zeros((240,320,3))\n",
    "print(\"sample shape --> \",sample1.shape)\n",
    "samp1 = np.moveaxis(sample1,[0,1,2],[2,0,1])\n",
    "print(\"new shape --> \",samp1.shape)\n",
    "imshow(samp1)\n",
    "im2 = color1(samp1)\n",
    "imshow(im2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samp1 == im2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train1 = data_utils.TensorDataset(xtrain2,ytrain2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_loader = data_utils.DataLoader(train1,batch_size=5,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, (i1,t1) in enumerate(train_loader):\n",
    "    print(type(i1))\n",
    "    temp1 = i1.numpy()\n",
    "    print(temp1.shape)\n",
    "    t = np.moveaxis(temp1,[0,1,2,3],[0,3,1,2])\n",
    "    imshow(t[0])\n",
    "    t[0] = np.fliplr(t[0])\n",
    "    imshow(t[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from imgaug import augmenters as iaa\n",
    "t = np.random.uniform(1.0,1.5)\n",
    "print(t)\n",
    "seq1 = iaa.Sequential([\n",
    "    iaa.Fliplr(0.5),\n",
    "    #iaa.Affine(scale=(t,t),rotate=(-5,5))\n",
    "    #iaa.Multiply((0.8,1.2))\n",
    "\n",
    "])\n",
    "seq = seq1.to_deterministic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new3 = np.moveaxis(xtest1,[0,1,2,3],[0,3,1,2])\n",
    "new1 = seq.augment_images(new3)\n",
    "new2 = seq.augment_images(ytest1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ht1 = xtest1[1]\n",
    "ht1 = np.moveaxis(ht1,[0,1,2],[2,0,1])\n",
    "imshow(ht1)\n",
    "imshow(new1[1])\n",
    "imshow(new2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#new2[0] = ytest1[0]/1.4933\n",
    "print(new2[0])\n",
    "dep1 = new2[0].copy()/1.4933\n",
    "print(\"depth here\")\n",
    "print(dep1)\n",
    "imshow(dep1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dep1[126][85]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
